# TabM: Simple and Strong Tabular Deep Learning via Shared Ensembles

- **원문**: `TABM.pdf`
- **저자**: (ICLR 2025) — 상세 저자 정보는 PDF 참조
- **주요 키워드**: Tabular DL, BatchEnsemble, Deep Ensembling, Efficiency

## 핵심 요약

TabM은 간단한 MLP 기반 모델과 파라미터 효율적인 앙상블 기법을 결합해 테이블 데이터에서 높은 성능과 효율을 동시에 달성하는 방법입니다. BatchEnsemble 계열 기법으로 대부분의 가중치를 공유하면서도 여러 예측을 생성하여 GBDT 대비 경쟁력 있는 성능을 보이며, 기존 주목받던 어텐션/검색 기반 구조보다 학습·추론 효율이 우수합니다.

## 주요 아이디어

1. **공유 가중치 앙상블**: 동일 MLP를 기반으로 다중 전문가를 구성하고, 각 구성원은 소수의 랭크-1 파라미터만 추가로 사용해 다양성을 확보합니다.
2. **다중 예측 평균**: 각 객체에 대해 여러 예측을 생성한 뒤 평균을 취해 개별 예측의 과적합을 완화하고 일반화를 강화합니다.
3. **대규모 벤치마크 평가**: 성능 순위, 분포, 학습 시간, 추론 처리량 등 네 가지 관점에서 기존 Tabular DL/GBDT와 비교하여 우수한 효율-성능 균형을 보고합니다.
4. **학습 다이나믹 분석**: 앙상블 구성원별 훈련 진행을 관찰하여, 전체 앙상블 기준으로 조기 종료하는 것이 최적 성능을 가져온다는 점을 확인합니다.

## 참고 자료

- BatchEnsemble (Wen et al., 2020) 요약.
- Tabular 데이터셋 벤치마크 목록.
